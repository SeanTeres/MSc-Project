{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user-pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchxrayvision\\utils.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, ConcatDataset, WeightedRandomSampler, Subset, DataLoader\n",
    "import os\n",
    "import torchxrayvision as xrv\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import pydicom\n",
    "from torchxrayvision.datasets import XRayCenterCrop\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix\n",
    "import helpers, train_utils, classes\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir_1 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1'\n",
    "metadata_1 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1/FileDatabaseWithRadiology.xlsx')\n",
    "dicom_dir_2 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2'\n",
    "metadata_2 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2/Database_Training-2024.08.28.xlsx')\n",
    "\n",
    "d1 = classes.DICOMDataset1(dicom_dir=dicom_dir_1, metadata_df=metadata_1, target_size=224) \n",
    "d2 = classes.DICOMDataset2(dicom_dir=dicom_dir_2, metadata_df=metadata_2, target_size=224)\n",
    "\n",
    "# Split datasets and store indices\n",
    "train_indices_d1, val_indices_d1, test_indices_d1 = helpers.split_dataset(d1)\n",
    "train_indices_d2, val_indices_d2, test_indices_d2 = helpers.split_dataset(d2)\n",
    "\n",
    "# Save indices for later use\n",
    "split_indices = {\n",
    "    'd1': {'train': train_indices_d1, 'val': val_indices_d1, 'test': test_indices_d1},\n",
    "    'd2': {'train': train_indices_d2, 'val': val_indices_d2, 'test': test_indices_d2}\n",
    "}\n",
    "\n",
    "label = 'Profusion'\n",
    "d1.set_target(target_label=label, target_size=224)\n",
    "d2.set_target(target_label=label, target_size=224)\n",
    "\n",
    "train_d1 = Subset(d1, train_indices_d1)\n",
    "val_d1 = Subset(d1, val_indices_d1)\n",
    "test_d1 = Subset(d1, test_indices_d1)\n",
    "\n",
    "train_d2 = Subset(d2, train_indices_d2)\n",
    "val_d2 = Subset(d2, val_indices_d2)\n",
    "test_d2 = Subset(d2, test_indices_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTHS: (824, 3296)\n",
      "LENGTHS: (26, 103)\n",
      "LENGTHS: (599, 2396)\n",
      "LENGTHS: (19, 75)\n"
     ]
    }
   ],
   "source": [
    "def create_weighted_sampler(dataset, target_label):\n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount([label for _, label in dataset])\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = [class_weights[label] for _, label in dataset]\n",
    "\n",
    "    # Create a weighted sampler\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "    return sampler\n",
    "\n",
    "# Create the base datasets\n",
    "train_d1 = Subset(d1, train_indices_d1)\n",
    "train_d2 = Subset(d2, train_indices_d2)\n",
    "\n",
    "# Define augmentations\n",
    "augmentations_list = [\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "]\n",
    "\n",
    "# Create augmented datasets\n",
    "augmented_train_d1 = classes.AugmentedDataset(base_dataset=train_d1, augmentations_list=augmentations_list)\n",
    "augmented_train_d2 = classes.AugmentedDataset(base_dataset=train_d2, augmentations_list=augmentations_list)\n",
    "\n",
    "    # Create dataloaders\n",
    "train_loader_d1, train_aug_loader_d1, val_loader_d1, test_loader_d1 = helpers.create_dataloaders(\n",
    "    train_d1, augmented_train_d1, val_d1, test_d1, batch_size=32, target=label\n",
    ")\n",
    "\n",
    "train_loader_d2, train_aug_loader_d2, val_loader_d2, test_loader_d2 = helpers.create_dataloaders(\n",
    "    train_d2, augmented_train_d2, val_d2, test_d2, batch_size=32, target=label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-all\").to(device)\n",
    "model.classifier = classes.BaseClassifier(in_features=1024\n",
    "                                          \n",
    "                                          )\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    814\n",
      "0.0    364\n",
      "Name: Profusion Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_label = 'Profusion Label'\n",
    "\n",
    "print(train_d1.dataset.metadata_df[target_label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_FLoss(train, target_label):\n",
    "    \"\"\"\n",
    "    Compute a single alpha value for binary focal loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    class_counts = train.dataset.metadata_df[target_label].value_counts()\n",
    "    class_counts = class_counts.sort_index()\n",
    "\n",
    "    # Ensure class counts exist\n",
    "    if len(class_counts) < 2:\n",
    "        raise ValueError(\"Dataset must contain both positive (1) and negative (0) samples.\")\n",
    "\n",
    "    minority = class_counts[0]  # Assuming class 0 is the minority\n",
    "    majority = class_counts[1]  # Assuming class 1 is the majority\n",
    "\n",
    "    # Compute alpha as the proportion of the negative class\n",
    "    alpha = minority / (majority + minority)\n",
    "\n",
    "    return np.round(alpha, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31\n"
     ]
    }
   ],
   "source": [
    "print(get_alpha_FLoss(train_d1, 'Profusion Label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_weight(train, target_label):\n",
    "    \"\"\"Compute pos_weight for BCEWithLogitsLoss.\"\"\"\n",
    "    class_counts = train.dataset.metadata_df[target_label].value_counts()\n",
    "    class_counts = class_counts.sort_index()\n",
    "\n",
    "    if len(class_counts) < 2:\n",
    "        raise ValueError(\"Dataset must contain both positive (1) and negative (0) samples.\")\n",
    "\n",
    "    N_pos = class_counts[1]  # Positive class count\n",
    "    N_neg = class_counts[0]  # Negative class count\n",
    "\n",
    "    pos_weight = torch.tensor([N_neg / N_pos], dtype=torch.float32)  # Must be a tensor\n",
    "\n",
    "    return pos_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4472])\n"
     ]
    }
   ],
   "source": [
    "print(compute_pos_weight(train_d1, 'Profusion Label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Resolution Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir_1 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1'\n",
    "metadata_1 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1/FileDatabaseWithRadiology.xlsx')\n",
    "dicom_dir_2 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2'\n",
    "metadata_2 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2/Database_Training-2024.08.28.xlsx')\n",
    "\n",
    "d1 = classes.DICOMDataset1(dicom_dir=dicom_dir_1, metadata_df=metadata_1, target_size=224) \n",
    "d2 = classes.DICOMDataset2(dicom_dir=dicom_dir_2, metadata_df=metadata_2, target_size=224)\n",
    "\n",
    "# Split datasets and store indices\n",
    "train_indices_d1, val_indices_d1, test_indices_d1 = helpers.split_dataset(d1)\n",
    "train_indices_d2, val_indices_d2, test_indices_d2 = helpers.split_dataset(d2)\n",
    "\n",
    "# Save indices for later use\n",
    "split_indices = {\n",
    "    'd1': {'train': train_indices_d1, 'val': val_indices_d1, 'test': test_indices_d1},\n",
    "    'd2': {'train': train_indices_d2, 'val': val_indices_d2, 'test': test_indices_d2}\n",
    "}\n",
    "\n",
    "label = 'Profusion'\n",
    "d1.set_target(target_label=label, target_size=512)\n",
    "d2.set_target(target_label=label, target_size=224)\n",
    "\n",
    "train_d1 = Subset(d1, train_indices_d1)\n",
    "val_d1 = Subset(d1, val_indices_d1)\n",
    "test_d1 = Subset(d1, test_indices_d1)\n",
    "\n",
    "train_d2 = Subset(d2, train_indices_d2)\n",
    "val_d2 = Subset(d2, val_indices_d2)\n",
    "test_d2 = Subset(d2, test_indices_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512]) 0\n",
      "torch.Size([1, 224, 224]) 1\n"
     ]
    }
   ],
   "source": [
    "model_512 = xrv.models.ResNet(weights=\"resnet50-res512-all\")\n",
    "model_224 = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "\n",
    "\n",
    "img, label = train_d1[12]\n",
    "img_2, label_2 = train_d2[12]\n",
    "\n",
    "print(img.shape, label)\n",
    "print(img_2.shape, label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = model_224.features(img_2.unsqueeze(0))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "model_512.features2 = model_512.features\n",
    "\n",
    "y = model_512.features2(img.unsqueeze(0))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classes\n",
    "model_512.classifier = classes.BaseClassifier512(in_features=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "res = model_512.classifier(y)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "labels = [True, False, True, True, False, True, False]\n",
    "preds = [True, True, True, True, True, True, True]\n",
    "\n",
    "print(cohen_kappa_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
