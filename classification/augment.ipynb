{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user-pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchxrayvision\\utils.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, ConcatDataset, WeightedRandomSampler, Subset, DataLoader\n",
    "import os\n",
    "import torchxrayvision as xrv\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import pydicom\n",
    "from torchxrayvision.datasets import XRayCenterCrop\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix\n",
    "import helpers, train_utils, classes\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir_1 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1'\n",
    "metadata_1 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1/FileDatabaseWithRadiology.xlsx')\n",
    "dicom_dir_2 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2'\n",
    "metadata_2 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2/Database_Training-2024.08.28.xlsx')\n",
    "\n",
    "d1 = classes.DICOMDataset1(dicom_dir=dicom_dir_1, metadata_df=metadata_1, target_size=224) \n",
    "d2 = classes.DICOMDataset2(dicom_dir=dicom_dir_2, metadata_df=metadata_2, target_size=224)\n",
    "\n",
    "# Split datasets and store indices\n",
    "train_indices_d1, val_indices_d1, test_indices_d1 = helpers.split_dataset(d1)\n",
    "train_indices_d2, val_indices_d2, test_indices_d2 = helpers.split_dataset(d2)\n",
    "\n",
    "# Save indices for later use\n",
    "split_indices = {\n",
    "    'd1': {'train': train_indices_d1, 'val': val_indices_d1, 'test': test_indices_d1},\n",
    "    'd2': {'train': train_indices_d2, 'val': val_indices_d2, 'test': test_indices_d2}\n",
    "}\n",
    "\n",
    "label = 'Profusion'\n",
    "d1.set_target(target_label=label, target_size=224)\n",
    "d2.set_target(target_label=label, target_size=224)\n",
    "\n",
    "train_d1 = Subset(d1, train_indices_d1)\n",
    "val_d1 = Subset(d1, val_indices_d1)\n",
    "test_d1 = Subset(d1, test_indices_d1)\n",
    "\n",
    "train_d2 = Subset(d2, train_indices_d2)\n",
    "val_d2 = Subset(d2, val_indices_d2)\n",
    "test_d2 = Subset(d2, test_indices_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of D1: 1178\n",
      "Length of D2: 857\n",
      "None \n",
      "\n",
      "1.0    830\n",
      "0.0    348\n",
      "Name: TBA-TBU Label, dtype: int64 \n",
      "\n",
      "1.0    814\n",
      "0.0    364\n",
      "Name: Profusion Label, dtype: int64 \n",
      "\n",
      "1.0    693\n",
      "0.0    485\n",
      "Name: Profusion and TBA-TBU Label, dtype: int64 \n",
      "\n",
      "1.0    951\n",
      "0.0    227\n",
      "Name: Profusion or TBA-TBU Label, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of D1: {len(d1)}')\n",
    "print(print(f'Length of D2: {len(d2)}'), \"\\n\")\n",
    "\n",
    "print(d1.metadata_df['TBA-TBU Label'].value_counts(), \"\\n\")\n",
    "print(d1.metadata_df['Profusion Label'].value_counts(), \"\\n\")\n",
    "print(d1.metadata_df['Profusion and TBA-TBU Label'].value_counts(), \"\\n\")\n",
    "print(d1.metadata_df['Profusion or TBA-TBU Label'].value_counts(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    766\n",
      "1.0    412\n",
      "Name: TBA-TBU Label, dtype: int64 \n",
      "\n",
      "1.0    814\n",
      "0.0    364\n",
      "Name: Profusion Label, dtype: int64 \n",
      "\n",
      "0.0    848\n",
      "1.0    330\n",
      "Name: Profusion and TBA-TBU Label, dtype: int64 \n",
      "\n",
      "1.0    896\n",
      "0.0    282\n",
      "Name: Profusion or TBA-TBU Label, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1_b = classes.DICOMDataset1_b(dicom_dir=dicom_dir_1, metadata_df=metadata_1, target_size=224)\n",
    "d1_b.set_target(target_label=label, target_size=224)\n",
    "\n",
    "print(d1_b.metadata_df['TBA-TBU Label'].value_counts(), \"\\n\")\n",
    "print(d1_b.metadata_df['Profusion Label'].value_counts(), \"\\n\")\n",
    "print(d1_b.metadata_df['Profusion and TBA-TBU Label'].value_counts(), \"\\n\")\n",
    "print(d1_b.metadata_df['Profusion or TBA-TBU Label'].value_counts(), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated label distributions:\n",
      "\n",
      "TBA-TBU Label:\n",
      "0.0    766\n",
      "1.0    412\n",
      "Name: TBA-TBU Label, dtype: int64 \n",
      "\n",
      "Profusion Label:\n",
      "1.0    814\n",
      "0.0    364\n",
      "Name: Profusion Label, dtype: int64 \n",
      "\n",
      "Profusion and TBA-TBU Label:\n",
      "0.0    848\n",
      "1.0    330\n",
      "Name: Profusion and TBA-TBU Label, dtype: int64 \n",
      "\n",
      "Profusion or TBA-TBU Label:\n",
      "1.0    896\n",
      "0.0    282\n",
      "Name: Profusion or TBA-TBU Label, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find rows where either doctor is missing\n",
    "missing_doctors_mask = (d1_b.metadata_df['strDoctor1'].isna()) & (d1_b.metadata_df['strDoctor2'].isna())\n",
    "\n",
    "# Set all labels to 0 for these rows\n",
    "d1_b.metadata_df.loc[missing_doctors_mask, 'TBA-TBU Label'] = 0\n",
    "d1_b.metadata_df.loc[missing_doctors_mask, 'Profusion Label'] = 0\n",
    "d1_b.metadata_df.loc[missing_doctors_mask, 'Profusion and TBA-TBU Label'] = 0\n",
    "d1_b.metadata_df.loc[missing_doctors_mask, 'Profusion or TBA-TBU Label'] = 0\n",
    "\n",
    "# Print updated label distributions\n",
    "print(\"Updated label distributions:\")\n",
    "print(\"\\nTBA-TBU Label:\")\n",
    "print(d1_b.metadata_df['TBA-TBU Label'].value_counts(), \"\\n\")\n",
    "print(\"Profusion Label:\")\n",
    "print(d1_b.metadata_df['Profusion Label'].value_counts(), \"\\n\")\n",
    "print(\"Profusion and TBA-TBU Label:\")\n",
    "print(d1_b.metadata_df['Profusion and TBA-TBU Label'].value_counts(), \"\\n\")\n",
    "print(\"Profusion or TBA-TBU Label:\")\n",
    "print(d1_b.metadata_df['Profusion or TBA-TBU Label'].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1178\n",
      "Filtered dataset size: 964\n",
      "Rows with both doctors missing: 214\n",
      "\n",
      "Label counts in removed rows:\n",
      "TBA-TBU Label: 0.0    214\n",
      "Name: TBA-TBU Label, dtype: int64\n",
      "Profusion Label: 0.0    214\n",
      "Name: Profusion Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows were actually filtered out\n",
    "print(\"Original dataset size:\", len(d1_b.metadata_df))\n",
    "print(\"Filtered dataset size:\", len(filtered_d1b))\n",
    "\n",
    "# Check how many rows have both doctors missing\n",
    "both_missing = d1_b.metadata_df[\n",
    "    (d1_b.metadata_df['strDoctor1'].isna()) & \n",
    "    (d1_b.metadata_df['strDoctor2'].isna())\n",
    "].shape[0]\n",
    "print(\"Rows with both doctors missing:\", both_missing)\n",
    "\n",
    "# Check if the filtered rows had any positive labels\n",
    "rows_removed = d1_b.metadata_df[\n",
    "    (d1_b.metadata_df['strDoctor1'].isna()) & \n",
    "    (d1_b.metadata_df['strDoctor2'].isna())\n",
    "]\n",
    "print(\"\\nLabel counts in removed rows:\")\n",
    "print(\"TBA-TBU Label:\", rows_removed['TBA-TBU Label'].value_counts())\n",
    "print(\"Profusion Label:\", rows_removed['Profusion Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_sampler(dataset, target_label):\n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount([label for _, label in dataset])\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = [class_weights[label] for _, label in dataset]\n",
    "\n",
    "    # Create a weighted sampler\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "    return sampler\n",
    "\n",
    "# Create the base datasets\n",
    "train_d1 = Subset(d1, train_indices_d1)\n",
    "train_d2 = Subset(d2, train_indices_d2)\n",
    "\n",
    "# Define augmentations\n",
    "augmentations_list = [\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "]\n",
    "\n",
    "# Create augmented datasets\n",
    "augmented_train_d1 = classes.AugmentedDataset(base_dataset=train_d1, augmentations_list=augmentations_list)\n",
    "augmented_train_d2 = classes.AugmentedDataset(base_dataset=train_d2, augmentations_list=augmentations_list)\n",
    "\n",
    "    # Create dataloaders\n",
    "train_loader_d1, train_aug_loader_d1, val_loader_d1, test_loader_d1 = helpers.create_dataloaders(\n",
    "    train_d1, augmented_train_d1, val_d1, test_d1, batch_size=32, target=label\n",
    ")\n",
    "\n",
    "train_loader_d2, train_aug_loader_d2, val_loader_d2, test_loader_d2 = helpers.create_dataloaders(\n",
    "    train_d2, augmented_train_d2, val_d2, test_d2, batch_size=32, target=label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-all\").to(device)\n",
    "model.classifier = classes.BaseClassifier(in_features=1024\n",
    "                                          \n",
    "                                          )\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = 'Profusion Label'\n",
    "\n",
    "print(train_d1.dataset.metadata_df[target_label].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_FLoss(train, target_label):\n",
    "    \"\"\"\n",
    "    Compute a single alpha value for binary focal loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    class_counts = train.dataset.metadata_df[target_label].value_counts()\n",
    "    class_counts = class_counts.sort_index()\n",
    "\n",
    "    # Ensure class counts exist\n",
    "    if len(class_counts) < 2:\n",
    "        raise ValueError(\"Dataset must contain both positive (1) and negative (0) samples.\")\n",
    "\n",
    "    minority = class_counts[0]  # Assuming class 0 is the minority\n",
    "    majority = class_counts[1]  # Assuming class 1 is the majority\n",
    "\n",
    "    # Compute alpha as the proportion of the negative class\n",
    "    alpha = minority / (majority + minority)\n",
    "\n",
    "    return np.round(alpha, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_alpha_FLoss(train_d1, 'Profusion Label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_weight(train, target_label):\n",
    "    \"\"\"Compute pos_weight for BCEWithLogitsLoss.\"\"\"\n",
    "    class_counts = train.dataset.metadata_df[target_label].value_counts()\n",
    "    class_counts = class_counts.sort_index()\n",
    "\n",
    "    if len(class_counts) < 2:\n",
    "        raise ValueError(\"Dataset must contain both positive (1) and negative (0) samples.\")\n",
    "\n",
    "    N_pos = class_counts[1]  # Positive class count\n",
    "    N_neg = class_counts[0]  # Negative class count\n",
    "\n",
    "    pos_weight = torch.tensor([N_neg / N_pos], dtype=torch.float32)  # Must be a tensor\n",
    "\n",
    "    return pos_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_pos_weight(train_d1, 'Profusion Label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Resolution Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_dir_1 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1'\n",
    "metadata_1 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 1/FileDatabaseWithRadiology.xlsx')\n",
    "dicom_dir_2 = 'C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2'\n",
    "metadata_2 = pd.read_excel('C:/Users/user-pc/Masters/MSc - Project/MBOD_Datasets/Dataset 2/Database_Training-2024.08.28.xlsx')\n",
    "\n",
    "d1 = classes.DICOMDataset1(dicom_dir=dicom_dir_1, metadata_df=metadata_1, target_size=224) \n",
    "d2 = classes.DICOMDataset2(dicom_dir=dicom_dir_2, metadata_df=metadata_2, target_size=224)\n",
    "\n",
    "# Split datasets and store indices\n",
    "train_indices_d1, val_indices_d1, test_indices_d1 = helpers.split_dataset(d1)\n",
    "train_indices_d2, val_indices_d2, test_indices_d2 = helpers.split_dataset(d2)\n",
    "\n",
    "# Save indices for later use\n",
    "split_indices = {\n",
    "    'd1': {'train': train_indices_d1, 'val': val_indices_d1, 'test': test_indices_d1},\n",
    "    'd2': {'train': train_indices_d2, 'val': val_indices_d2, 'test': test_indices_d2}\n",
    "}\n",
    "\n",
    "label = 'Profusion'\n",
    "d1.set_target(target_label=label, target_size=512)\n",
    "d2.set_target(target_label=label, target_size=224)\n",
    "\n",
    "train_d1 = Subset(d1, train_indices_d1)\n",
    "val_d1 = Subset(d1, val_indices_d1)\n",
    "test_d1 = Subset(d1, test_indices_d1)\n",
    "\n",
    "train_d2 = Subset(d2, train_indices_d2)\n",
    "val_d2 = Subset(d2, val_indices_d2)\n",
    "test_d2 = Subset(d2, test_indices_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_512 = xrv.models.ResNet(weights=\"resnet50-res512-all\")\n",
    "model_224 = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "\n",
    "\n",
    "img, label = train_d1[12]\n",
    "img_2, label_2 = train_d2[12]\n",
    "\n",
    "print(img.shape, label)\n",
    "print(img_2.shape, label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = 'C:/Users/user-pc/Masters/MSc - Project/prof_and_tb-D2-BCE-OS_best_model_val_kappa.pth'  # Replace with the actual path to your checkpoint file\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint file found: {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Checkpoint file not found: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "str_224 = \"densenet121-res224-all\"\n",
    "str_512 = \"resnet50-res512-all\"\n",
    "\n",
    "model_x = xrv.models.DenseNet(weights=str_224)\n",
    "model_x.classifier = classes.BaseClassifier(in_features=1024)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model_x.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = 'C:/Users/user-pc/Masters/MSc - Project/prof_and_tb-D2-BCE-OS_best_model_val_kappa.pth'  # Replace with the actual path to your checkpoint file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_x.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state dictionary\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Access the epoch\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "print(f\"Model and optimizer state loaded from epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations_list = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.RandomRotation(15)\n",
    "])\n",
    "\n",
    "print(augmentations_list)\n",
    "\n",
    "img, label = train_d2[12]\n",
    "\n",
    "# Apply augmentations\n",
    "augmented_img = augmentations_list(img)\n",
    "\n",
    "print(augmented_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(augmented_img.squeeze(0), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentations\n",
    "augmentations_list = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "    transforms.\n",
    "])\n",
    "\n",
    "# Create augmented datasets\n",
    "d1_aug = classes.DICOMDataset1(dicom_dir=dicom_dir_1, metadata_df=metadata_1, transform=augmentations_list)\n",
    "d2_aug = classes.DICOMDataset2(dicom_dir=dicom_dir_2, metadata_df=metadata_2, transform=augmentations_list)\n",
    "\n",
    "train_d1 = Subset(d1, train_indices_d1)\n",
    "train_aug_d1 = Subset(d1_aug, train_indices_d1)\n",
    "\n",
    "val_d1 = Subset(d1, val_indices_d1)\n",
    "test_d1 = Subset(d1, test_indices_d1)\n",
    "\n",
    "train_d2 = Subset(d2, train_indices_d2)\n",
    "train_aug_d2 = Subset(d2_aug, train_indices_d2)\n",
    "\n",
    "val_d2 = Subset(d2, val_indices_d2)\n",
    "test_d2 = Subset(d2, test_indices_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_persp = transforms.RandomPerspective(distortion_scale=0.1, p=1.0, fill=1)\n",
    "\n",
    "img, label = train_d2[12]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img.squeeze(0), cmap='gray')\n",
    "plt.title(\"Original\")\n",
    "\n",
    "# Apply random perspective\n",
    "perspective_img = rand_persp(img)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(perspective_img.squeeze(0), cmap='gray')\n",
    "plt.title(\"Random Perspective\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augs_to_rand_apply = transforms.RandomApply(torch.nn.ModuleList([\n",
    "    transforms.CenterCrop(np.round(img.shape[1]*0.9).astype(int)),\n",
    "    transforms.RandomRotation(degrees=(-10,10))\n",
    "]), p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = augs_to_rand_apply(img)\n",
    "\n",
    "plt.imshow(trans.squeeze(0), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def salt_and_pepper_noise_tensor(image, prob=0.02):\n",
    "    \"\"\"\n",
    "    Apply salt-and-pepper noise to a PyTorch tensor image.\n",
    "    \n",
    "    :param image: PyTorch tensor of shape (C, H, W), values in [0,1].\n",
    "    :param prob: Probability of a pixel being affected.\n",
    "    :return: Noisy image tensor.\n",
    "    \"\"\"\n",
    "    assert image.dim() == 3, \"Input must be a 3D tensor (C, H, W)\"\n",
    "    \n",
    "    noisy_image = image.clone()  # Clone to avoid modifying original image\n",
    "    \n",
    "    # Generate random noise mask\n",
    "    rand_tensor = torch.rand_like(image)  # Random values between [0,1]\n",
    "\n",
    "    # Apply Salt (white pixels)\n",
    "    noisy_image[rand_tensor < prob / 2] = 1.0  # If image is in [0,1], use 255.0 for [0,255]\n",
    "\n",
    "    # Apply Pepper (black pixels)\n",
    "    noisy_image[rand_tensor > 1 - prob / 2] = 0.0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_image = salt_and_pepper_noise_tensor(img, prob=0.1)\n",
    "\n",
    "plt.imshow(noisy_image.squeeze(0), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformation pipeline\n",
    "augs_to_rand_apply = transforms.RandomApply([\n",
    "    # transforms.CenterCrop(np.round(224 * 0.9).astype(int)),  # Example crop\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),  \n",
    "    transforms.Lambda(lambda img: salt_and_pepper_noise_tensor(img, prob=0.05)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05))\n",
    "], p=1)  # 50% chance of applying the transformations\n",
    "\n",
    "\n",
    "res = augs_to_rand_apply(img)\n",
    "\n",
    "plt.imshow(res.squeeze(0), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.memory_summary(device=None, abbreviated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming training for checkpointed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "str_224 = \"densenet121-res224-all\"\n",
    "str_512 = \"resnet50-res512-all\"\n",
    "\n",
    "model_x = xrv.models.DenseNet(weights=str_224)\n",
    "model_x.classifier = classes.BaseClassifier(in_features=1024)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model_x.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = 'C:/Users/user-pc/Masters/MSc - Project/tb-D2-BCE-OS-aug_p_50_30_final_model.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_x.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state dictionary\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Access the epoch\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "print(f\"Model and optimizer state loaded from epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
